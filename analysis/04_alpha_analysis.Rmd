---
title: 'Analysis of the rate of forgetting in Session 1'
author: "Maarten van der Velde"
date: "Last updated: `r Sys.time()`"
output:
  html_notebook:
    smart: no
    toc: yes
  html_document:
    df_print: paged
    toc: yes
editor_options: 
  chunk_output_type: inline
---


# Setup
```{r}
library(dplyr)
library(ggplot2)
library(purrr)
library(scales)
library(tidyr)
library(tikzDevice)
library(fst)

theme_set(theme_bw())

source("bayes_funs.R")
```

---

# Session 1

## Lab data

```{r}
lab_1_rl_final_alpha <- read.fst(file.path("..", "data", "processed", "lab", "session1", "session1_final_alpha_lab.fst"))
```


### Bayesian model of fact alpha

Model priors:
```{r}
mu_0 <- 0.3 # mean (b)
kappa_0 <- 1 # number of observations/precision (c)
a_0 <- 3 # shape of Gamma (g)
b_0 <- 0.2 # rate of Gamma (h)
```


We only include observations from participants who encountered the fact at least 3 times, since it is only at that point that adjustment of alpha starts.

```{r}
alpha <- lab_1_rl_final_alpha %>%
  filter(reps >= 3)
```


#### Estimates per fact

The plot below shows for all 30 facts how the posterior predictive distribution changes with each additional observation (prior is shown in blue; most recent posterior in red; the red dotted line marks the model's point estimate).

```{r, fig.height=8}
fact_res <- alpha %>%
  nest(-fact_id) %>%
  mutate(res = map(data, ~ run_bayes_model_incremental(.$final_alpha)))

par(mfrow = c(6,5), mar = c(1,1,1,1), cex = 1)
invisible(map(fact_res$res, ~ plot_posterior_predictive_stacked(.$mu_n, .$kappa_n, .$a_n, .$b_n)))
```

#### Distribution of predictions

The peak of each posterior predicitive distribution (i.e., its mean) is used as a point estimate for initial alpha.
How are these values distributed?
```{r}
estimates_lab <- fact_res %>%
  unnest(res) %>% 
  group_by(fact_id) %>%
  slice(n())

summary(estimates_lab$mu_n)
boxplot(estimates_lab$mu_n, horizontal = TRUE)
```


#### How does the model's point estimate develop over time?

What is the effect of additional observations on the mean of the posterior predictive distribution?

In most cases the mean remains more or less constant after about 15 observations.
```{r, fig.height=8}
par(mfrow = c(6, 5), mar = c(1,1,1,1), cex = 1)

invisible(map(fact_res$res, ~ plot(.$mu_n, type = "l", lwd = 2, xlim = c(0, 50), ylim = c(0.2, 0.5))))
```
---

#### Visualising change in the model's point estimate over time

If we visualise the size of the change in mean after each model update (figure below), we can see that, aside from some large changes at the start, the changes very quickly fall within quite a narrow range, centered around zero.

```{r, fig.height=8}
par(mfrow = c(6, 5), mar = c(1,1,1,1), cex = 0.9)

tolerance <- 0.00496

invisible(map(fact_res$res, function(x) {
  delta <- x$mu_n - lag(x$mu_n)
  plot(delta, type = "l", xlim = c(0, 50), ylim = c(-0.03, 0.03))
  abline(h = c(-tolerance, tolerance), lty = 2, col = "blue")
  points(delta, col = ifelse(abs(delta) > tolerance, "red", "darkgreen"), pch = 16)
  }))
```

In the figure below all 30 change trajectories are combined.
Again, it is clear that changes converge quite quickly on a narrow range.
(For context: the current dataset contains at least `r fact_res$res %>% map_dbl(nrow) %>% min` observations per fact.)

```{r, fig.height=5}
delta <- fact_res$res[[1]]$mu_n - lag(fact_res$res[[1]]$mu_n)
plot(delta, type = "n", xlim = c(0, 50), ylim = c(-0.03, 0.03))

invisible(map(fact_res$res, function(x) {
  delta <- x$mu_n - lag(x$mu_n)
  lines(delta, lwd = 0.5)
  points(delta, col = ifelse(abs(delta) > tolerance, "red", "darkgreen"), pch = 16)
  }))

abline(h = c(-tolerance, tolerance), lty = 2, lwd = 2, col = "blue")
```


#### Estimating changes in stability

We can get a better idea of how the stability of an estimate changes by running the same model many times with the data in a different order.

Here I run each fact model 250 times on randomly ordered observations.

```{r}
n_sim <- 250

fact_res_perm <- list()

for (i in 1:n_sim) {
  
  fact_res_perm[[i]] <- alpha %>%
    sample_frac() %>%
    nest(-fact_id) %>%
    mutate(iteration = i,
           res = map(data, ~ run_bayes_model_incremental(.$final_alpha))) %>%
    select(iteration, fact_id, res)
  
}

fact_res_perm <- bind_rows(fact_res_perm) %>%
  arrange(fact_id, iteration)
```

The figure below shows that the order in which new observations are introduced to the model can be quite influential. 
The end result when all observations are included is always the same, regardless of the order in which they were added, but right up to that point there is some variability that results from the order in which observations were added.

It is also interesting to see the difference between models: some are much more sensitive to order than others, reflecting a larger disagreement between the individual observations.
This is a different way of showing the uncertainty in the final posterior predictive distribution, which you can also see by the amount of spread and the height of that distribution (see previous plots).

```{r, fig.height=8}
fact_res_perm_plotdat <- fact_res_perm %>%
  unnest() %>%
  group_by(iteration, fact_id) %>%
  mutate(observation = 1:n()) %>%
  ungroup() %>%
  select(iteration, fact_id, observation, mu_n)


ggplot(fact_res_perm_plotdat, aes(x = observation, y = mu_n, group = iteration)) +
  geom_line(alpha = 0.2) +
  facet_wrap(~ fact_id) +
  labs(title = "Evolution of facts' predicted rate of forgetting as the number of observations grows",
       subtitle = "Each fact model run 250 times on shuffled data",
       x = "Number of observations",
       y = "Predicted alpha")
```


The analysis of the size of the change can be repeated in this permutation setup.
The plot below shows the absolute size of the change in the model estimate.
The red dotted line indicates a 0.00496 tolerance threshold.

```{r, fig.height=8}
fact_res_perm_plotdat_delta <- fact_res_perm_plotdat %>%
  group_by(iteration, fact_id) %>%
  mutate(delta = mu_n - lag(mu_n)) %>%
  ungroup() 


fact_res_perm_plotdat_delta %>%
  filter(!is.na(delta)) %>%
  ggplot(aes(x = observation, y = abs(delta), group = iteration)) +
  geom_line(alpha = 0.2) +
  geom_hline(yintercept = tolerance, linetype = 2, colour = "red") +
  ylim(0, 0.05) +
  facet_wrap(~ fact_id) +
  labs(title = "Size of changes in facts' predicted rate of forgetting as the number of observations grows",
       subtitle = "Each fact model run 250 times on shuffled data",
       x = "Number of observations",
       y = "Size of the change in predicted alpha")
```


Using this information, we can calculate the proportion of adjustments that are within the specified tolerance window at a given point.
The following plot shows, given a number of observations, what proportion of rate of forgetting adjustments are smaller than or equal to the tolerance threshold of `r tolerance`.
There is a horizontal black line at 95%.
Where this line is crossed, 95% of all adjustments at this point fall within the tolerance threshold.

On average, a model needs about 35 observations to reach this level.
Given that each fact has at least `r fact_res$res %>% map_dbl(nrow) %>% min` observations and on average `r fact_res$res %>% map_dbl(nrow) %>% mean` observations, we are roughly at that point, which means that we would not expect much marginal improvement if more data was collected.

```{r, fig.height= 5}
fact_res_perm_plotdat_delta %>%
  filter(!is.na(delta)) %>%
  mutate(within_tolerance = abs(delta) <= tolerance) %>%
  group_by(observation) %>%
  mutate(within_tolerance_mean = mean(within_tolerance)) %>%
  group_by(fact_id, observation, within_tolerance_mean) %>%
  summarise(within_tolerance = mean(within_tolerance)) %>%
  ungroup() %>%
  ggplot(aes(x = observation, y = within_tolerance, colour = as.factor(fact_id))) +
  geom_line(alpha = 0.6) +
  geom_line(aes(x = observation, y = within_tolerance_mean), colour = "black", size = 1) +
  geom_hline(yintercept = 0.95) +
  scale_y_continuous(labels = percent_format()) +
  labs(title = "Percentage of changes in rate of forgetting that fall within tolerance (0.00496)",
       subitle = "Black line shows mean percentage across facts",
       x = "Number of observations",
       y = "Percentage of changes within tolerance",
       colour = "Fact")
```



---


## MTurk data

```{r}
mturk_rl_final_alpha <- read.fst(file.path("..", "data", "processed", "mturk", "session1", "session1_final_alpha_mturk.fst"))
```


### Bayesian model of fact alpha

Model priors:
```{r}
mu_0 <- 0.3 # mean (b)
kappa_0 <- 1 # number of observations/precision (c)
a_0 <- 3 # shape of Gamma (g)
b_0 <- 0.2 # rate of Gamma (h)
```


We only include observations from participants who encountered the fact at least 3 times, since it is only at that point that adjustment of alpha starts.

```{r}
alpha_m <- mturk_rl_final_alpha %>%
  filter(reps >= 3)
```


#### Estimates per fact

The plot below shows for all 30 facts how the posterior predictive distribution changes with each additional observation (prior is shown in blue; most recent posterior in red; the red dotted line marks the model's point estimate).

```{r, fig.height=8}
fact_res_m <- alpha_m %>%
  nest(-fact_id) %>%
  mutate(res = map(data, ~ run_bayes_model_incremental(.$final_alpha)))

par(mfrow = c(6,5), mar = c(1,1,1,1), cex = 1)
invisible(map(fact_res_m$res, ~ plot_posterior_predictive_stacked(.$mu_n, .$kappa_n, .$a_n, .$b_n)))
```

#### Distribution of predictions

The peak of each posterior predicitive distribution (i.e., its mean) is used as a point estimate for initial alpha.
How are these values distributed?
```{r}
estimates_mturk <- fact_res_m %>%
  unnest(res) %>% 
  group_by(fact_id) %>%
  slice(n())

summary(estimates_mturk$mu_n)
boxplot(estimates_mturk$mu_n, horizontal = TRUE)
```


#### How does the model's point estimate develop over time?

What is the effect of additional observations on the mean of the posterior predictive distribution?

In most cases the mean remains more or less constant after about 15 observations.
```{r, fig.height=8}
par(mfrow = c(6, 5), mar = c(1,1,1,1), cex = 1)

invisible(map(fact_res_m$res, ~ plot(.$mu_n, type = "l", lwd = 2, xlim = c(0, 50), ylim = c(0.2, 0.5))))
```
---

#### Visualising change in the model's point estimate over time

If we visualise the size of the change in mean after each model update (figure below), we can see that, aside from some large changes at the start, the changes very quickly fall within quite a narrow range, centered around zero.

```{r, fig.height=8}
par(mfrow = c(6, 5), mar = c(1,1,1,1), cex = 0.9)

tolerance <- 0.00496

invisible(map(fact_res_m$res, function(x) {
  delta <- x$mu_n - lag(x$mu_n)
  plot(delta, type = "l", xlim = c(0, 50), ylim = c(-0.03, 0.03))
  abline(h = c(-tolerance, tolerance), lty = 2, col = "blue")
  points(delta, col = ifelse(abs(delta) > tolerance, "red", "darkgreen"), pch = 16)
  }))
```

In the figure below all 30 change trajectories are combined.
Again, it is clear that changes converge quite quickly on a narrow range.
(For context: the current dataset contains at least `r fact_res_m$res %>% map_dbl(nrow) %>% min` observations per fact.)

```{r, fig.height=5}
delta <- fact_res_m$res[[1]]$mu_n - lag(fact_res_m$res[[1]]$mu_n)
plot(delta, type = "n", xlim = c(0, 50), ylim = c(-0.03, 0.03))

invisible(map(fact_res_m$res, function(x) {
  delta <- x$mu_n - lag(x$mu_n)
  lines(delta, lwd = 0.5)
  points(delta, col = ifelse(abs(delta) > tolerance, "red", "darkgreen"), pch = 16)
  }))

abline(h = c(-tolerance, tolerance), lty = 2, lwd = 2, col = "blue")
```


#### Estimating changes in stability

As before, here I run each fact model 250 times on randomly ordered observations to see how sensitive each model is to order changes and how quickly adjustments fall within a threshold.

```{r}
n_sim <- 250

fact_res_m_perm <- list()

for (i in 1:n_sim) {
  
  fact_res_m_perm[[i]] <- alpha_m %>%
    sample_frac() %>%
    nest(-fact_id) %>%
    mutate(iteration = i,
           res = map(data, ~ run_bayes_model_incremental(.$final_alpha))) %>%
    select(iteration, fact_id, res)
  
}

fact_res_m_perm <- bind_rows(fact_res_m_perm) %>%
  arrange(fact_id, iteration)
```



```{r, fig.height=8}
fact_res_m_perm_plotdat <- fact_res_m_perm %>%
  unnest() %>%
  group_by(iteration, fact_id) %>%
  mutate(observation = 1:n()) %>%
  ungroup() %>%
  select(iteration, fact_id, observation, mu_n)


ggplot(fact_res_m_perm_plotdat, aes(x = observation, y = mu_n, group = iteration)) +
  geom_line(alpha = 0.2) +
  facet_wrap(~ fact_id) +
  labs(title = "Evolution of facts' predicted rate of forgetting as the number of observations grows",
       subtitle = "Each fact model run 250 times on shuffled data",
       x = "Number of observations",
       y = "Predicted alpha")
```


```{r, fig.height=8}
fact_res_m_perm_plotdat_delta <- fact_res_m_perm_plotdat %>%
  group_by(iteration, fact_id) %>%
  mutate(delta = mu_n - lag(mu_n)) %>%
  ungroup() 


fact_res_m_perm_plotdat_delta %>%
  filter(!is.na(delta)) %>%
  ggplot(aes(x = observation, y = abs(delta), group = iteration)) +
  geom_line(alpha = 0.2) +
  geom_hline(yintercept = tolerance, linetype = 2, colour = "red") +
  ylim(0, 0.05) +
  facet_wrap(~ fact_id) +
  labs(title = "Size of changes in facts' predicted rate of forgetting as the number of observations grows",
       subtitle = "Each fact model run 250 times on shuffled data",
       x = "Number of observations",
       y = "Size of the change in predicted alpha")
```


The plot below shows that, on average, a model needs about 31 observations to reach this level -- slightly less than with the lab data.
Given that each fact has at least `r fact_res_m$res %>% map_dbl(nrow) %>% min` observations and on average `r fact_res_m$res %>% map_dbl(nrow) %>% mean` observations, we are well beyond that point, which means that here we would also not expect much marginal improvement if more data was collected.


```{r, fig.height= 5}
fact_res_m_perm_plotdat_delta %>%
  filter(!is.na(delta)) %>%
  mutate(within_tolerance = abs(delta) <= tolerance) %>%
  group_by(observation) %>%
  mutate(within_tolerance_mean = mean(within_tolerance)) %>%
  group_by(fact_id, observation, within_tolerance_mean) %>%
  summarise(within_tolerance = mean(within_tolerance)) %>%
  ungroup() %>%
  ggplot(aes(x = observation, y = within_tolerance, colour = as.factor(fact_id))) +
  geom_line(alpha = 0.6) +
  geom_line(aes(x = observation, y = within_tolerance_mean), colour = "black", size = 1) +
  geom_hline(yintercept = 0.95) +
  scale_y_continuous(labels = percent_format()) +
  labs(title = "Percentage of changes in rate of forgetting that fall within tolerance (0.00496)",
       subitle = "Black line shows mean percentage across facts",
       x = "Number of observations",
       y = "Percentage of changes within tolerance",
       colour = "Fact")
```



## Combined

Plot of fact alpha predictions:
```{r}
theme_set(theme_light(base_size = 14))

predictions <- bind_rows(mutate(estimates_lab, dataset = "Lab"),
                         mutate(estimates_mturk, dataset = "MTurk")) %>%
  group_by(fact_id) %>%
  mutate(difference = diff(mu_n)) %>%
  ungroup()

domain_predictions <- predictions %>%
  group_by(dataset) %>%
  summarise(mu_n = mean(mu_n))

source("geom_flat_violin.R")

predictions_plot <- ggplot(predictions, aes(y = mu_n, x = dataset)) +
  geom_hline(yintercept = 0.3, lty = 2) +
  geom_flat_violin_mirrored(data = filter(predictions, dataset == "Lab"), scale = "width", width = 0.8) +
  geom_flat_violin(data = filter(predictions, dataset == "MTurk"), scale = "width", width = 0.8) +
  geom_line(aes(group = fact_id, colour = as.factor(fact_id)), alpha = 0.5, lty = 1) +
  geom_point(aes(colour = as.factor(fact_id)), size = 3) +
  geom_point(data = domain_predictions, size = 2, shape = 5, position = position_nudge(x = c(-0.25, 0.25))) +
  guides(colour = FALSE) +
  scale_color_viridis_d() +
  scale_y_continuous(limits = c(0.225, 0.50), breaks = c(0.25, 0.30, 0.35, 0.40, 0.45, 0.50), minor_breaks = NULL) +
  labs(x = NULL,
       y = "Predicted rate of forgetting")

predictions_plot
ggsave("../output/session1/predictions_dist_fact.pdf", device = "pdf", width = 2.5, height = 3)


tikz(file ="../output/session1/predictions_dist_fact.tex", width = 2.5, height = 3)
predictions_plot +
  theme_light()
dev.off()


domain_predictions
```

Spearman rank correlation:
```{r}
cor(estimates_lab$mu_n, estimates_mturk$mu_n, method = "spearman")

cor.test(x = estimates_lab$mu_n, y = estimates_mturk$mu_n, method = "spearman")
```



# Session info
```{r}
sessionInfo()
```

