---
title: "Test of H4: The rate of forgetting estimate converges on its final value more quickly when its initial value was predicted using learning history"
author: "Maarten van der Velde"
date: "Last updated: `r Sys.time()`"
output:
  html_notebook:
    smart: no
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
---

# Overview

This notebook contains the analysis of Hypothesis 4, as preregistered at https://osf.io/vwg6u/:

> The absolute distance between intermediate estimates and the final estimate of the rate of forgetting for a particular learner/fact combination during a learning session will converge on zero (i.e., fall within a specified boundary above zero) within fewer adjustments when the initial estimate of the rate of forgetting was based on the most predictive combination(s) of learner and/or fact history than when the initial estimate of the rate of forgetting was set at the default value of 0.3.

# Setup

## Load packages
```{r}
library(BayesFactor)
library(dplyr)
library(forcats)
library(ggplot2)
library(purrr)
library(readr)
library(stringr)
library(tidyr)
library(tikzDevice)
library(lme4)
library(lmerTest)
library(fst)

theme_set(theme_light(base_size = 14) +
  theme(strip.text = element_text(colour = "black")))

knitr::opts_chunk$set(fig.width=16, fig.height=16) 
```

## Load data
```{r}
responses_lab_2_with_model_params <- read.fst(file.path("..", "data", "processed", "lab", "session2", "session2_rl2_with_alpha_lab.fst"))
responses_mturk_2_with_model_params <- read.fst(file.path("..", "data", "processed", "mturk", "session2", "session2_rl2_with_alpha_mturk.fst"))

fix_condition_labels <- function(x) {
  rowwise(x) %>%
  mutate(condition = str_replace(condition, "-and-", " & ") %>%
           str_replace("student", "learner") %>%
           str_to_title()) %>%
  ungroup() %>%
  mutate(condition = fct_relevel(condition, "Fact & Learner", after = Inf)) # Move F&L level to the end
}

block2_with_model_params <- bind_rows(mutate(responses_lab_2_with_model_params, dataset = "Lab"),
                                      mutate(responses_mturk_2_with_model_params, dataset = "MTurk")) %>%
  fix_condition_labels()
```

## Subset data
The preregistration specified 22 April 2019 as the cutoff date for data collection.
We reached the minimum of 25 participants per condition in the lab sample before this date, but not in the Mechanical Turk sample.
For that reason data collection on MTurk continued until there were at least 25 participants per condition (14 May 2019) and was stopped only then.
The analysis reported in the paper is based only on the data from before the cutoff date, but this notebook also reports the same analysis done on the full dataset. 

```{r}
subjects_cutoff <- read_csv(file.path("..", "data", "processed", "subjects_before_cutoff.csv"))

block_2_full <- block2_with_model_params 

block_2_cutoff <- block2_with_model_params %>%
  right_join(subjects_cutoff, by = "subject")
```


# Convergence of the estimate

Each trial has a value `alpha` that expresses the estimated rate of forgetting of the fact at the start of the trial.
The rate of forgetting estimate starts at the predicted value (which depends on the condition).
It is then updated after each repetition of the fact to better match the observed responses.

```{r}
alpha_change <- block_2_full %>%
  group_by(dataset, subject, condition, fact_id) %>%
  arrange(subject, condition, fact_id, repetition) %>%
  mutate(d_alpha = alpha - lag(alpha)) %>%
  mutate(d_alpha = ifelse(is.na(d_alpha), 0, d_alpha)) %>%
  mutate(abs_d_alpha = abs(d_alpha)) %>%
  mutate(final_alpha = tail(alpha,1)) %>%
  ungroup() %>%
  select(dataset, subject, condition, fact_id, repetition, alpha, d_alpha, abs_d_alpha, final_alpha)
```

The plot below visualises the development of each alpha estimate (every learner/fact combination is represented by a line).
It shows several things:

- The initial rate of forgetting estimate differs depending on the condition: in conditions with a domain-wide prediction (Default and Domain) the same value is always used, while in conditions with more individualised predictions (Fact and/or Learner) the starting value differs between facts and/or learners.
- The rate of forgetting estimate is only changed *after* the third presentation of the fact. Up to that point it is always a horizontal line.
- Because of the way the scheduling algorithm works, there is invariably a plume towards the top right: facts with a lower rate of forgetting are repeated less frequently than facts with a higher rate of forgetting.

```{r, fig.width=12, fig.height=6}
ggplot(alpha_change, aes(x = repetition, y = alpha, group = interaction(subject, fact_id))) +
  facet_grid(dataset ~ condition, labeller = label_wrap_gen()) +
  geom_line(alpha = 0.1) +
  labs(x = "Presentation",
       y = "Rate of forgetting")
```


Our question is whether the model can find the correct rate of forgetting more quickly in conditions that use a predicted value as their starting point.

The plot below shows the *change* in the rate of forgetting estimate compared to the previous presentation.
Note that changes are capped at 0.0496 in both directions.
It appears that many of the changes are as large as possible.

```{r, fig.width=12, fig.height=6}
ggplot(alpha_change, aes(x = repetition, y = d_alpha, group = interaction(subject, fact_id))) +
  facet_grid(dataset ~ condition, labeller = label_wrap_gen()) +
  geom_line(alpha = 0.1) +
  labs(x = "Presentation",
       y = "Change in rate of forgetting")
```


The distribution of changes (shown below, excluding the first three presentations since they can never have changes) confirms that many changes are either as large as possible, or almost zero.
The dotted horizontal lines show the boundaries of what we consider to be the convergence zone (0.00496 on either side of zero).
This plot also gives an indication of the balance between upward and downward changes.
It is especially apparent that changes in the Default condition are biased towards upward changes, which makes sense given the general diffulty of the material.
It looks like the changes may be more balanced in other conditions.

```{r, fig.width=12, fig.height=6}
alpha_change %>%
  filter(repetition > 3) %>%
ggplot(aes(x = condition, y = d_alpha)) +
  facet_grid(dataset ~ ., labeller = label_wrap_gen()) +
  geom_jitter(aes(colour = condition), width = 0.1, height = 0, alpha = 0.1) +
  geom_violin(fill = NA) +
  geom_hline(yintercept = c(0.00496, -0.00496), lty = 2) +
  scale_colour_brewer(type = "qual", palette = 7) +
  guides(colour = FALSE) +
  labs(x = NULL,
       y = "Change in rate of forgetting",
       caption = "Excluding the first 3 trials, in which RoF cannot change.")
```


```{r}
alpha_change %>%
  filter(repetition > 3) %>%
ggplot(aes(x = d_alpha)) +
  facet_grid(dataset ~ condition, labeller = label_wrap_gen()) +
  geom_histogram(aes(fill = abs_d_alpha <= 0.00496), binwidth = 0.002) +
  labs(x = "Change in rate of forgetting",
       fill = "Within\nconvergence\nzone",
       caption = "Excluding the first 3 trials, in which RoF cannot change.")

```




The number of presentations per fact, which we expected to drop with better prediction, in fact seems to be *higher* in predictive conditions, if anything:

```{r fig.width=12}
n_reps <- block_2_full %>%
  group_by(dataset, subject, condition, fact_id) %>%
  summarise(repetitions = max(repetition))

ggplot(filter(n_reps, repetitions >= 4), aes(x = condition, y = repetitions)) +
  facet_grid(dataset ~ .) +
  geom_violin() +
  geom_jitter(aes(colour = condition), width = 0.05, height = 0, alpha = 0.5) +
  scale_colour_brewer(type = "qual", palette = 7) +
  guides(colour = FALSE) +
  labs(x = NULL,
       y = "Repetitions",
       caption = "Excluding facts with fewer than 4 repetitions")
```

This might simply be an effect of the high difficulty of items overall: in most cases, if the rate of forgetting is estimated more accurately from the start, the item *should* be repeated more often since it is correctly assessed as more difficult.
We would expect that for the easiest facts (of which there are only a few), prediction does decrease repetitions compared to the default condition.

As a sanity check, verify that the number of repetitions scales with the final alpha estimate:
```{r fig.width=12}
n_reps_by_alpha <- block_2_full %>%
  group_by(dataset, subject, condition, fact_id) %>%
  summarise(repetitions = max(repetition),
            alpha = alpha[which.max(repetition)])

ggplot(filter(n_reps_by_alpha, repetitions >= 4), aes(x = alpha, y = repetitions, colour = condition)) +
  facet_grid(dataset ~ condition, labeller = label_wrap_gen()) +
  geom_jitter(alpha = 0.5) +
  scale_colour_brewer(type = "qual", palette = 7) +
  guides(colour = FALSE)
```


The plot below shows only the number of repetitions for cases in which the final alpha is lower than 0.3, and hints at an improvement (particularly in the MTurk data).
Starting off with a lower-frequency repetition schedule indeed seems to lead to fewer repetitions overall.

```{r}
n_reps_by_alpha <- block_2_full %>%
  group_by(dataset, subject, condition, fact_id) %>%
  summarise(repetitions = max(repetition),
            alpha = alpha[which.max(repetition)])

ggplot(filter(n_reps_by_alpha, repetitions >= 4, alpha < 0.3), aes(x = condition, y = repetitions, colour = condition)) +
  facet_grid(dataset ~ .) +
  geom_violin() +
  geom_jitter(width = 0.05, height = 0, alpha = 0.5) +
  scale_colour_brewer(type = "qual", palette = 7) +
  guides(colour = FALSE) +
  labs(x = NULL,
       y = "Repetitions",
       caption = "Only showing facts with alpha < 0.3; excluding facts with fewer than 4 repetitions")
```

Indeed, an exploratory Bayesian ANOVA shows strong evidence for an effect of condition on the number of repetitions among low-alpha items (but no effect of dataset):
```{r}
n_reps_by_alpha_low <- n_reps_by_alpha %>%
  ungroup() %>%
  mutate_if(is_character, as.factor) %>%
  filter(repetitions >= 4, alpha < 0.3)

anovaBF(
  repetitions ~ condition * dataset,
  data = n_reps_by_alpha_low,
  progress = FALSE
)
```

Follow-up t-tests indicate that adapation in general has a beneficial effect...
```{r}
ttestBF(filter(n_reps_by_alpha_low, condition == "Default")$repetitions, filter(n_reps_by_alpha_low, condition != "Default")$repetitions)
```

... but that there is only anecdotal evidence supporting a more individualised adaptation over a domain-wide adaptation:
```{r}
ttestBF(filter(n_reps_by_alpha_low, condition == "Domain")$repetitions, filter(n_reps_by_alpha_low, condition %in% c("Fact & Learner", "Fact", "Learner"))$repetitions)
```



If we plot the cumulative percentage of converged estimates, we see that estimates in the default condition are more likely to converge than those in other conditions---not what we expected.  


```{r fig.width=12}
# convergence <- alpha_change %>%
#   group_by(dataset, subject, condition, fact_id) %>%
#   filter(abs_d_alpha > 0.00496) %>% # Keep cases in which the adjustment is outside the window
#   summarise(convergence_point = max(repetition) + 1) %>%
#   ungroup() %>%
#   mutate(dataset = as.factor(dataset),
#          subject = as.factor(subject),
#          condition = as.factor(condition),
#          fact_id = as.factor(fact_id))


# convergence <- alpha_change %>%
#   group_by(dataset, subject, condition, fact_id) %>%
#   mutate(within_boundary = abs_d_alpha <= 0.00496) %>%
#   mutate(total_reps = max(repetition)) %>%
#   filter(!within_boundary) %>%
#   summarise(total_reps = total_reps[1], convergence_point = max(repetition) + 1, final_alpha = alpha[which.max(repetition)]) %>%
#   mutate(convergence_point = ifelse(convergence_point > total_reps, NA, convergence_point))

convergence <- alpha_change %>%
  group_by(dataset, subject, condition, fact_id) %>%
  filter(max(repetition) > 3) %>%
  mutate(within_boundary = abs_d_alpha <= 0.00496) %>%
  mutate(total_reps = max(repetition)) %>%
  summarise(total_reps = total_reps[1],
            convergence_point = last(repetition[!within_boundary]) + 1) %>%
  mutate(convergence_point = ifelse(convergence_point > total_reps, NA, convergence_point))


convergence_cumulative <- convergence %>%
  group_by(dataset, condition) %>%
  count(convergence_point) %>%
  complete(nesting(dataset, condition), convergence_point = c(0:max(convergence$convergence_point, na.rm = T), NA), fill = list(n = 0)) %>%
  mutate(perc_converged = n / sum(n)) %>%
  mutate(perc_converged = cumsum(perc_converged)) %>%
  ungroup() %>%
  arrange(dataset, condition, convergence_point) %>%
  fill(perc_converged)
  

convergence_cumulative_combined <- convergence %>%
  group_by(condition) %>%
  count(convergence_point) %>%
  complete(nesting(condition), convergence_point = c(0:max(convergence$convergence_point, na.rm = T), NA), fill = list(n = 0)) %>%
  mutate(perc_converged = n / sum(n)) %>%
  mutate(perc_converged = cumsum(perc_converged)) %>%
  ungroup() %>%
  arrange(condition, convergence_point) %>%
  fill(perc_converged)

```



```{r fig.width=12}
convergence_cumulative_combined %>%
  filter(!is.na(convergence_point)) %>%
  ggplot(aes(x = convergence_point, y = perc_converged, colour = condition, lty = condition)) +
  # facet_wrap(~ dataset) +
  geom_line(size = 1) +
  scale_colour_brewer(type = "qual", palette = 7) +
  scale_y_continuous(labels = scales::percent_format()) +
  expand_limits(x = 0, y = 0) +
  labs(x = "Repetition",
       y = "Estimates converged",
       colour = "Condition",
       lty = "Condition")
       # caption = "Convergence means that there are no more adjustments larger than 0.00496")

ggsave("../output/cum_perc_converged.pdf", device = "pdf", width = 5, height = 3)
```

Make the plot shown in the paper:
```{r}
convergence_cumulative_combined_tex <- convergence_cumulative_combined
levels(convergence_cumulative_combined_tex$condition)[5] <- "Fact \\& Learner"

tikz(file = "../output/cum_perc_converged.tex", width = 4.75, height = 2)

convergence_cumulative_combined_tex %>%
  filter(!is.na(convergence_point)) %>%
  ggplot(aes(x = convergence_point, y = perc_converged, colour = condition, lty = condition)) +
  geom_line(size = 1) +
  scale_colour_brewer(type = "qual", palette = 7) +
  scale_y_continuous(labels = scales::percent_format(suffix = "\\%")) +
  expand_limits(x = 0, y = 0) +
  labs(x = "Repetition",
       y = "Estimates converged",
       colour = "Condition",
       lty = "Condition") +
  theme_light()

dev.off()
```



What proportion of estimates converged in each condition?
```{r}
convergence_cumulative %>%
  group_by(dataset, condition) %>%
  filter(!is.na(convergence_point)) %>%
  summarise(n_converged = sum(n),
            perc_converged = max(perc_converged))
```



For the estimates that did converge, plot the distribution of convergence points by condition:
```{r}
convergence %>%
  filter(!is.na(convergence_point)) %>%
  ggplot(aes(x = condition, y = convergence_point)) +
  # facet_grid(dataset ~ ., labeller = label_wrap_gen()) +
  geom_jitter(aes(colour = condition), width = 0.1, height = 0, alpha = 0.1) +
  geom_violin(fill = NA) +
  expand_limits(y = 0) +
  scale_colour_brewer(type = "qual", palette = 7) +
  guides(colour = FALSE) +
  labs(x = NULL,
       y = "Convergence point")

ggsave("../output/convergence_point.pdf", device = "pdf", width = 5, height = 3)

```

Make the plot shown in the paper:
```{r}
convergence_tex <- convergence
levels(convergence_tex$condition)[5] <- "Fact \\& Learner"

tikz(file = "../output/convergence_point.tex", width = 4.75, height = 2)

convergence_tex %>%
  filter(!is.na(convergence_point)) %>%
  ggplot(aes(x = condition, y = convergence_point)) +
  # facet_grid(dataset ~ ., labeller = label_wrap_gen()) +
  geom_jitter(aes(colour = condition), width = 0.1, height = 0, alpha = 0.1) +
  geom_violin(fill = NA) +
  expand_limits(y = 0) +
  scale_colour_brewer(type = "qual", palette = 7) +
  guides(colour = FALSE) +
  labs(x = NULL,
       y = "Convergence point") +
  theme_light()

dev.off()
```



As preregistered, we conduct a Bayesian ANOVA testing the effect of condition on the convergence point.

There is very strong evidence for this model compared to a null model.
```{r}
convergence_dat <- convergence %>%
  ungroup() %>%
  filter(!is.na(convergence_point)) %>%
  mutate_if(is.character, as.factor) %>%
  mutate(fact_id = as.factor(fact_id))

bf_convergence <- lmBF(
  formula = convergence_point ~ condition * dataset + subject + fact_id,
  whichRandom = c("subject", "fact_id"),
  data = convergence_dat,
  progress = FALSE
)
  
bf_convergence  

1/bf_convergence
```
To check whether we should test the two datasets separately, compare the first model to one without the interaction between condition and dataset.
This comparison shows that there is strong evidence for the model without the interaction and against the model with interaction.
```{r}
bf_convergence_nointeraction <- lmBF(
  formula = convergence_point ~ condition + dataset + subject + fact_id,
  whichRandom = c("subject", "fact_id"),
  data = convergence_dat,
  progress = FALSE
)
  
bf_convergence_nointeraction / bf_convergence
```

A model that leaves out dataset altogether is supported even more strongly by the evidence, showing that we can assume no effect of dataset on the convergence point.
```{r}
bf_convergence_nodataset <- lmBF(
  formula = convergence_point ~ condition + subject + fact_id,
  whichRandom = c("subject", "fact_id"),
  data = convergence_dat,
  progress = FALSE
)

bf_convergence_nodataset / bf_convergence_nointeraction
```

Compare to the maximal model:
```{r}
bf_convergence_nodataset / bf_convergence
```


Compare the preferred simpler model to a model without condition effect:
```{r}
bf_convergence_nocondition <-  lmBF(
  formula = convergence_point ~ subject + fact_id,
  whichRandom = c("subject", "fact_id"),
  data = convergence_dat,
  progress = FALSE
)

bf_convergence_nocondition / bf_convergence_nodataset 
```

```{r}
bf_convergence_nocondition / bf_convergence_nointeraction
bf_convergence_nocondition / bf_convergence
```


Although it's not the best model, we can sample from the posterior of the model *with* condition to see what it would predict:
```{r}
samples <- posterior(bf_convergence_nodataset, iterations = 1000, progress = FALSE)
summary(samples[,1:6])
```




---

# Exploratory: the effect of the RoF estimate on convergence

## Convergence vs. no convergence

```{r}
convergence_traces <- convergence %>%
  mutate(converges = !is.na(convergence_point)) %>%
  left_join(alpha_change, by = c("dataset", "subject", "condition", "fact_id"))
```

### Trajectories
```{r}
ggplot(convergence_traces, aes(x = repetition, y = alpha, colour = interaction(subject, fact_id))) +
  facet_grid(converges ~ condition) +
  geom_line(alpha = 0.1) +
  geom_point(alpha = 0.1) +
  guides(colour = FALSE) +
  labs(title = "Trajectories that do not converge (top) and those that do (bottom)",
       y = "Rate of forgetting")

ggplot(convergence_traces, aes(x = repetition, y = d_alpha, colour = interaction(subject, fact_id))) +
  facet_grid(converges ~ condition) +
  geom_line(alpha = 0.1) +
  geom_point(alpha = 0.1) +
  geom_hline(yintercept = c(-0.00496, 0.00496), lty = 2) +
  guides(colour = FALSE) +
  labs(title = "Trajectories that do not converge (top) and those that do (bottom)",
       y = "Change in rate of forgetting")

```


Indexed on the final value:
```{r}
convergence_traces %>%
  group_by(dataset, subject, condition, fact_id) %>%
  mutate(alpha_endaligned = alpha - final_alpha) %>%
  ggplot(aes(x = repetition, y = alpha_endaligned, colour = interaction(subject, fact_id))) +
  facet_grid(converges ~ condition) +
  geom_line(alpha = 0.1) +
  geom_point(alpha = 0.1) +
  guides(colour = FALSE) +
  labs(title = "Trajectories that do not converge (top) and those that do (bottom)",
       y = "Rate of forgetting (indexed on final value)")

```


### Convergence point ~ number of repetitions

The convergence point is by definition capped to the total number of repetitions a fact receives (at the latest, we can observe convergence at the final repetition).
But it is also quite clear that convergence rarely happens before the final repetition, which requires more than two small changes in a row.
Pearson's r: `r cor(convergence$convergence_point, convergence$total_reps, use = "complete.obs")`.
```{r}
ggplot(filter(convergence, !is.na(convergence_point)), aes(x = total_reps, y = convergence_point)) +
  geom_jitter(alpha = 0.2) +
  coord_equal() +
  geom_abline(slope = 1, intercept = 0, lty = 2)
```



Do changes in RoF become smaller as the number of repetitions increases?
As the plot below shows: yes, but up to a point. After about 7 repetitions the size of the change stays more or less constant.
```{r}
convergence_traces %>%
  group_by(repetition) %>%
  summarise(mean_change = mean(abs_d_alpha),
            se_change = plotrix::std.error(abs_d_alpha)) %>%
  ggplot(aes(x = repetition, y = mean_change)) +
  geom_point() +
  geom_errorbar(aes(ymin= mean_change - se_change, ymax = mean_change + se_change)) +
  labs(x = "Repetition",
       y = "Mean change in RoF estimate",
       caption = "Error bars show +/- 1 standard error of the mean")
```


### Convergence point ~ final RoF

The convergence point seems to depend on the final rate of forgetting: the higher the rate of forgetting, the later convergence happens.
Importantly, this trend seems to exist in all conditions.
```{r}
quantiles <- quantile(filter(distinct(convergence_traces, dataset, subject, condition, fact_id, converges, final_alpha), converges)$final_alpha,
                      probs = seq(0, 1, 0.2))

convergence_traces %>%
  filter(converges) %>%
  ungroup() %>%
  mutate(final_alpha_bin = cut(final_alpha, quantiles, include.lowest = TRUE)) %>%
  group_by(dataset, subject, condition, fact_id, final_alpha) %>%
  slice(n()) %>%
  ggplot(aes(x = final_alpha_bin, y = convergence_point)) +
  facet_grid(~ condition) +
  geom_jitter(aes(colour = final_alpha_bin), width = 0.2, alpha = 0.2) +
  geom_violin(fill = NA) +
  geom_boxplot(width = 0.2, outlier.shape = NA) +
  guides(colour = FALSE) +
  labs(x = "Final rate of forgetting (binned in quantiles)",
       y = "Convergence point") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
   
```

```{r}
convergence_final_rof <- distinct(convergence_traces, dataset, subject, condition, fact_id, final_alpha, converges, convergence_point) %>%
  ungroup() %>%
  filter(converges) %>%
  mutate(dataset = as.factor(dataset),
         subject = as.factor(subject),
         fact_id = as.factor(fact_id))

m_conv_final_rof <- lmer(convergence_point ~ final_alpha * condition  + (1 | subject) + (1 | fact_id), data = convergence_final_rof)

summary(m_conv_final_rof)
```

The lmer confirms a large, positive main effect of final rate of forgetting on convergence point, but no effect of, or interaction with, condition.

This implies that, regardless of the condition, convergence will happen sooner if the final estimate is lower.


Compare the final rate of forgetting distribution of estimates that did converge to those that did not.
```{r}
convergence_traces %>%
  distinct(dataset, subject, condition, fact_id, .keep_all = TRUE) %>%
  ggplot(aes(x = converges, y = final_alpha)) +
  facet_grid(~ condition) +
  geom_jitter(aes(colour = converges), width = 0.2, alpha = 0.2) +
  geom_violin(fill = NA) +
  geom_boxplot(width = 0.2, outlier.shape = NA) +
  guides(colour = FALSE)

```


```{r}
conv <- convergence_traces %>%
  distinct(dataset, subject, condition, fact_id, .keep_all = TRUE)

m_conv_final <- lmer(converges ~ final_alpha * condition + (1 | subject) + (1 | fact_id), data = conv)
summary(m_conv_final)
```


Takeaway:

- Estimates that do converge tend to have a higher final value than estimates that don't (which makes some sense, since there are more opportunities for adjustment/stabilising).
- For estimates that converge, convergence happens sooner when the final value is lower.
- Tradeoff: lower final value means less likely to converge, but when convergence does happen, it's faster. 


### Convergence point ~ initial RoF

Is the above also true for the initial rate of forgetting estimate?
Maybe, judging by the plot below (note that the quantiles are recalculated based on initial alpha values).
```{r}
quantiles <- quantile(filter(convergence_traces, converges, repetition == 1)$alpha,
                      probs = seq(0, 1, 0.2))

convergence_traces %>%
  filter(converges, repetition == 1) %>%
  ungroup() %>%
  mutate(alpha_bin = cut(alpha, quantiles, include.lowest = TRUE)) %>%
  ggplot(aes(x = alpha_bin, y = convergence_point)) +
  facet_grid(~ condition) +
  geom_jitter(aes(colour = alpha_bin), width = 0.2, alpha = 0.2) +
  geom_violin(fill = NA) +
  geom_boxplot(width = 0.2, outlier.shape = NA) +
  guides(colour = FALSE) +
  labs(x = "Initial rate of forgetting (binned in quantiles)",
       y = "Convergence point") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

```

```{r}
convergence_init_rof <- convergence_traces %>%
  ungroup() %>%
  filter(converges, repetition == 1) %>%
  mutate(dataset = as.factor(dataset),
         subject = as.factor(subject),
         fact_id = as.factor(fact_id))

m_conv_init_rof <- lmer(convergence_point ~ alpha * condition  + (1 | subject) + (1 | fact_id), data = convergence_init_rof)

summary(m_conv_init_rof)
```


The lmer confirms that the convergence point increases as a function of the initial rate of forgetting (`alpha`) in all conditions, even accounting for the interaction between the initial alpha and the condition.

Convergence seems to be about equally likely for low and high initial values.
```{r}
convergence_traces %>%
  distinct(dataset, subject, condition, fact_id, .keep_all = TRUE) %>%
  ggplot(aes(x = converges, y = alpha)) +
  facet_grid(~ condition) +
  geom_jitter(aes(colour = converges), width = 0.2, alpha = 0.2) +
  geom_violin(fill = NA) +
  geom_boxplot(width = 0.2, outlier.shape = NA) +
  guides(colour = FALSE)

```


The lack of an effect is confirmed by an lmer:
```{r}
m_conv_init <- lmer(converges ~ alpha * condition + (1 | subject) + (1 | fact_id), data = conv)
summary(m_conv_init)
```



We can surmise that the convergence point is sensitive to both the initial estimate and the final estimate.
When conditions have unequal initial estimates, as is the case here (the initial estimates in the adaptive conditions tend to be higher than those in the default condition), this confounds the results.

It is clear that the operationalisation of the convergence point is problematic: one would come to the conclusion that a lower initial estimate is always better, simply because that is rewarded in the current analysis.
We may want to think about a measure that is robust to this.

Ideas:

- Split the data on initial estimate (lower than 0.3 or higher than 0.3) and see if convergence improves below 0.3.
- ...



The lower convergence success in adaptive distributions may be because the final alpha might be higher in these conditions.
There seems to be a slightly lower final rate of forgetting in the default condition compared to the adaptive conditions, but this mostly disappears in pairwise comparisons (also when we only look at converged estimates), and is also not supported by a Bayesian lmBF, which slightly prefers a model without condition.
```{r}
p_final_rof <- ggplot(conv, aes(x = condition, y = final_alpha)) +
  geom_violin() +
  geom_jitter(alpha = 0.1, aes(colour = condition)) +
  geom_boxplot(outlier.shape = NA, width = 0.5) +
  labs(x = "Condition",
       y = "Final rate of forgetting") +
  guides(colour = FALSE)

tikz(file = "../output/final_rof_by_cond.tex", width = 4, height = 2.5)
p_final_rof +
  scale_x_discrete(labels = c("Default", "Domain", "Fact", "Learner", "Fact \\& Learner")) +
  theme_light()
dev.off()

p_final_rof
```

```{r}
summary(m_final_rof_cond <- lmer(final_alpha ~ condition + (1 | subject) + (1 | fact_id), data = conv))

bf_final_rof_cond <- lmBF(
  formula = final_alpha ~ condition + subject + fact_id,
  whichRandom = c("subject", "fact_id"),
  data = mutate(ungroup(conv), subject = as.factor(subject), fact_id = as.factor(fact_id)),
  progress = FALSE
)
  
bf_final_rof_nocond <- lmBF(
  formula = final_alpha ~ subject + fact_id,
  whichRandom = c("subject", "fact_id"),
  data = mutate(ungroup(conv), subject = as.factor(subject), fact_id = as.factor(fact_id)),
  progress = FALSE
)
  
bf_final_rof_cond / bf_final_rof_nocond 

```

Tangential: how similar are final rates of forgetting for specific facts between conditions?
Looks fairly consistent across conditions:
```{r}
conv %>%
  group_by(condition, fact_id) %>%
  summarise(alpha = mean(final_alpha),
            n = n()) %>%
  # mutate(rank = dense_rank(desc(alpha))) %>%
  ggplot(aes(x = condition, y = alpha, group = fact_id, colour = as.factor(fact_id))) +
  geom_line(alpha = 0.75) +
  geom_point(aes(size = n)) +
  guides(colour = FALSE)
```


---

Make diagnostic plots:


```{r}
library(patchwork)

# Change in rate of forgetting with each repetition
p_rof_change <- convergence_traces %>%
  group_by(repetition) %>%
  summarise(mean_change = mean(abs_d_alpha),
            se_change = plotrix::std.error(abs_d_alpha)) %>%
  ggplot(aes(x = repetition, y = mean_change)) +
  geom_point() +
  geom_errorbar(aes(ymin= mean_change - se_change, ymax = mean_change + se_change), na.rm = T) +
  geom_hline(yintercept = 0.00496, colour = "red", lty = 2) +
  labs(x = "Repetition",
       y = "Absolute rate of forgetting change")

  
# Convergence point ~ total number of repetitions
p_conv_reps <- ggplot(filter(convergence, !is.na(convergence_point)), aes(x = total_reps, y = convergence_point)) +
  geom_jitter(alpha = 0.2) +
  coord_equal() +
  geom_abline(slope = 1, intercept = 0, lty = 2) +
  labs(x = "Total repetitions",
      y = "Convergence point")


p_conv_point_initial <- ggplot(conv, aes(x = alpha, y = convergence_point, colour = condition, fill = condition)) +
  facet_grid(~ dataset) +
  geom_point(alpha = 0.1, na.rm = T) +
  geom_smooth(method = "lm", se = T, na.rm = T, size = .75) +
  # geom_rug(aes(y = NULL), sides = "b") +
  labs(x = "Initial rate of forgetting",
         y = "Convergence point")

p_conv_point_final <- ggplot(conv, aes(x = final_alpha, y = convergence_point, colour = condition, fill = condition)) +
  facet_grid(~ dataset) +
  geom_point(alpha = 0.1, na.rm = T) +
  geom_smooth(method = "lm", se = T, na.rm = T, size = .75) +
  # geom_rug(aes(y = NULL), sides = "b") +
  labs(x = "Final rate of forgetting",
         y = "Convergence point")

p_conv_prob_initial <- ggplot(conv, aes(x = alpha, y = as.numeric(converges), colour = condition, fill = condition)) +
  facet_grid(~ dataset) +
  geom_point(alpha = 0.1, na.rm = T) +
  geom_smooth(method = "glm",  method.args=list(family="binomial"), se = T, na.rm = T) +
  # geom_rug(aes(y = NULL), sides = "b") +
  ylim(0, 1) +
  labs(x = "Initial rate of forgetting",
       y = "Convergence probability")

p_conv_prob_final <- ggplot(conv, aes(x = final_alpha, y = as.numeric(converges), colour = condition, fill = condition)) +
  facet_grid(~ dataset) +
  geom_point(alpha = 0.1, na.rm = T) +
  geom_smooth(method = "glm",  method.args=list(family="binomial"), se = T, na.rm = T) +
  # geom_rug(aes(y = NULL), sides = "b") +
  ylim(0, 1) +
  labs(x = "Final rate of forgetting",
       y = "Convergence probability")


p_rof_change + p_conv_reps + p_conv_point_initial + p_conv_prob_initial + p_conv_point_final + p_conv_prob_final + plot_layout(ncol = 2, byrow = TRUE)

ggsave("../output/convergence_diagnostics.pdf", width = 12, height = 12)

```

Plot the RoF change plot for each condition separately:
```{r}
p_rof_change_cond <- convergence_traces %>%
  group_by(condition, repetition) %>%
  summarise(mean_change = mean(abs_d_alpha),
            se_change = plotrix::std.error(abs_d_alpha)) %>%
  ggplot(aes(x = repetition, y = mean_change, colour = condition)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(ymin= mean_change - se_change, ymax = mean_change + se_change), na.rm = T) +
  geom_hline(yintercept = 0.00496, colour = "red", lty = 2) +
  labs(x = "Repetition",
       y = "Absolute rate of forgetting change") +
  xlim(0,10) 

p_rof_change_cond
```


Also plot each diagnostic plot separately:
```{r}
tikz(file = "../output/rof_change.tex", width = 4, height = 2.5)
p_rof_change +
  theme_light()
dev.off()

tikz(file = "../output/conv_reps.tex", width = 2.5, height = 2.5)
p_conv_reps +
  theme_light()
dev.off()

tikz(file = "../output/conv_point_initial_rof.tex", width = 4, height = 2)
p_conv_point_initial +
  scale_color_discrete(labels = c("Default", "Domain", "Fact", "Learner", "Fact \\& Learner")) +
  scale_fill_discrete(labels = c("Default", "Domain", "Fact", "Learner", "Fact \\& Learner")) +
  theme_light()
dev.off()

tikz(file = "../output/conv_point_final_rof.tex", width = 4, height = 2)
p_conv_point_final +
  scale_color_discrete(labels = c("Default", "Domain", "Fact", "Learner", "Fact \\& Learner")) +
  scale_fill_discrete(labels = c("Default", "Domain", "Fact", "Learner", "Fact \\& Learner")) +
  theme_light()
dev.off()

tikz(file = "../output/conv_prob_initial_rof.tex", width = 4, height = 2)
p_conv_prob_initial +
  scale_color_discrete(labels = c("Default", "Domain", "Fact", "Learner", "Fact \\& Learner")) +
  scale_fill_discrete(labels = c("Default", "Domain", "Fact", "Learner", "Fact \\& Learner")) +
  theme_light()
dev.off()

tikz(file = "../output/conv_prob_final_rof.tex", width = 4, height = 2)
p_conv_prob_final +
  scale_color_discrete(labels = c("Default", "Domain", "Fact", "Learner", "Fact \\& Learner")) +
  scale_fill_discrete(labels = c("Default", "Domain", "Fact", "Learner", "Fact \\& Learner")) +
  theme_light()
dev.off()

```




---

# Session information
```{r}
sessionInfo()
```

